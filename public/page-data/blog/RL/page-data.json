{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/RL/","webpackCompilationHash":"3e7463e6478d3ac1be03","result":{"data":{"markdownRemark":{"html":"<p>From self driving cars, Space Invaders, and the game Go, reinforcement learning (RL) is how deep learning tackles the complex problems. In its most general case, reinforcement learning is any time an agent (usually a neural network), navigates an environment that it has some influence over and has rewards. Lets break the environment into parts. An environment is anything that has states like the game screen of an Atari game or the the x y coordinates of a agent in a maze. Furthermore, an agent having influence just means it can make actions that effect the environment, moving a set forward in a maze or steering a car are both examples of this. The rewards are a critical part of an agents learning and tell an agent whether it is doing good or bad. For example, if an agent is navigating a maze, it will receive an positive award if it reaches the finish line, but if it walks into a pit of fire, it will receive a negative award. These fundamentals is what makes RL learning function.</p>\n<p>However, there are different ways to implement this idea and the most successful is Q-learning. Q-learning works by giving each state a Q-value which give a measure of how good a state is. For example, if a agent is solving a maze, the finish will have a high Q-value since being at the finish is the best possible state to be in. A pit of fire will have a lot Q-value since it will kill the agent and send it back to the start of the maze. To calculate, the Q-value we use the Bellman Equation.</p>\n<p>This equation states that the Q-value for an action in a state is equal to the expected reward times the discount factor for each time step that that action produces. To simplify, to find the value of the action in a state we have to look what rewards we will get in the future for doing this. For example, if we are in a maze and we take a step towards the finish line, we anticipate a reward in the future assuming we take the best route. Hereâ€™s where the discounting factor gamma comes into play. The discounting factor is a number between 1 and 0 and is a measure of how far our agent will look ahead into the future. So, if we have a discounting factor of 0.9, and we will reach the finish the maze in three step if we move left, for our current state and the action of moving left will give a Q-value of 10*0.9^3=7.29. We discount future rewards since a reward in the future has the the potential to disappear, but a big enough reward will make it worth it and overcome the discounting factor.</p>\n<p>To train the network we use a modified Bellman Equation. Since we are always learning new information about the environment, we will have to constantly update our Q-value. To do this, we use the equation above. Once again we see the discounting factor, but a new coefficient has show up, the learning rate. This value simply tells how strongly do we change our Q-value with each new observation. We do not want to reset out Q-value entirely based of each new observation, and instead change it in small amounts. If we set the learning rate to 1, the old states cancel out and the new Q-value is whatever the most recent observation suggest. If we set the learning rate to 0, the new state will never update with new observations. Thus, we try to set our learning rate in the middle, so out network slowly updates over time. We can then train our network using the updated Q-value as the target. This will allow our agent to learn over time and find the Q-values of each state and action.</p>\n<p>Source:</p>\n<p>Deep Learning A-Z: Hands On Artificial Networks Course on Udemy found at <a href=\"https://www.udemy.com/deeplearning/\">https://www.udemy.com/deeplearning/</a></p>","frontmatter":{"title":"RL in RL (Reinforcement Learning in Real Life)"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/blog/RL/"}}}